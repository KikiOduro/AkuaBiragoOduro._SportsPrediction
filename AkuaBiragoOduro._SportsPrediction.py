# -*- coding: utf-8 -*-
"""Fifa22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OtjNBo5J3GtLbeKCW5ZWpelw9u7O0T_C

# Imports
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error, r2_score
import pickle
import joblib

"""Mounting Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Data Preprocessing"""

male_players = pd.read_csv('/content/drive/My Drive/FIFA23_PREDICTIONMODEL/male_players (legacy).csv')

pd.set_option('display.max_columns', None)

male_players.info()

male_players.head()

male_players.columns

male_players.describe()

print('The shape of this dataset is',male_players.shape[1])

"""Removing a percentage of null values in the dataset"""

threshold_percentage = 0.2  # 20% of the total rows
min_non_null_values = int(threshold_percentage * male_players.shape[0])
male_players.dropna(axis=1, thresh=min_non_null_values, inplace=True)

print('The shape of this dataset is',male_players.shape[1])

print(male_players.columns)

"""Listing useless variables"""

useless_columns = [
    "player_url",
    "player_id",
    "dob",
    "club_team_id",
    "long_name",
    "short_name",
    "real_face",
    "body_type",
    "club_jersey_number",
    "club_joined_date",
    "nationality_id",
    "player_face_url",
    "fifa_update",
    "fifa_update_date"
]

"""Dropping useless variables"""

new_players_22 = male_players.drop(columns=useless_columns)

print(new_players_22.columns)

new_players_22.info()

"""Splitting Numerical and Categorical variables in the dataset"""

categorical_data = pd.DataFrame()
numerical_data = pd.DataFrame()

for columns in new_players_22.columns:
    if new_players_22[columns].dtype == 'object':
        categorical_data[columns] = new_players_22[columns]
    else:
        numerical_data[columns] = new_players_22[columns]

categorical_data.info()

numerical_data.info()

"""Checking for Null values in the numerical dataframe"""

print(numerical_data.isnull().sum())

"""Replacing null values with the mean of each column (mean ffilling)


"""

numerical_data.ffill(axis=0, inplace=True)

print(numerical_data.isnull().sum())

numerical_data['release_clause_eur'].fillna(numerical_data['release_clause_eur'].mean(), inplace=True)
numerical_data['mentality_composure'].fillna(numerical_data['mentality_composure'].mean(), inplace=True)

print(numerical_data.isnull().sum())

"""Ffilling categorical values with the mode"""

categorical_data.ffill(axis=0, inplace=True)

print(categorical_data.isnull().sum())

"""Encoding categorical values using LabelEncoder due to its simplicity"""

encoder = LabelEncoder()

encoded_categroical_data = categorical_data.copy()

categorical_data = categorical_data.astype(str)

for columns in categorical_data.columns:
    encoded_categroical_data[columns] = encoder.fit_transform(categorical_data[columns])

# encoded_categorical_data = categorical_data.apply(encoder.fit_transform)

encoded_categroical_data.info()

encoded_categroical_data

clean_male_players_22_data = pd.concat([numerical_data, encoded_categroical_data], axis=1)

clean_male_players_22_data.info()

"""# Feature engineering"""

from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

"""Finding the correlation of the Quantitative variables"""

# qdf = new_players_22.select_dtypes(include=['int64','float64'])

# qdf.head()

clean_male_players_22_data.info()

clean_male_players_22_data['gk'] = pd.to_numeric(clean_male_players_22_data['gk'], errors='coerce')

corr_matrix = clean_male_players_22_data.corr()

corr_matrix

corr_matrix['overall']

"""Finding features that have a strong correlation with overall"""

corr_with_overall = corr_matrix['overall'].sort_values(ascending=False)
print("Features strongly correlated with overall rating:")
print(corr_with_overall)

"""Using the random forest classsifier to find the feature importance"""

#Choosing X and Y
X= clean_male_players_22_data.drop('overall',axis=1)
y=clean_male_players_22_data['overall']

X

y

#Using test_train split to split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

#Determining the feature importance of dependent variables using Random forest classifier

feature_impotance = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})

feature_impotance

feature_impotance = feature_impotance.sort_values('importance', ascending=False)
feature_impotance

top_10_features = corr_with_overall.iloc[1:21]

print("Top 10 features strongly correlated with overall rating:")
print(top_10_features)

#Picking the important features according to the feature importance and correlation matrix
selected_features=["movement_reactions","potential","passing","wage_eur","dribbling","value_eur","player_positions"]

"""# Training, Testing and Evaluation


"""

X = clean_male_players_22_data[selected_features]
y = pd.DataFrame(clean_male_players_22_data['overall'])

X

y

#Scaling using the standard scaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""## Training - RandomForestRegressor"""

rf_regressor = RandomForestRegressor(n_estimators=112, random_state=42)

rf_regressor.fit(X_train, y_train)

initial_prediction = rf_regressor.predict(X_test)
initial_prediction

initial_rmse = np.sqrt(mean_squared_error(y_test, initial_prediction))
print("Initial RMSE:", initial_rmse)

param_grid = {
    'n_estimators': [100, 120],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

scoring = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)
scoring

grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid,
                           scoring=scoring, cv=5, verbose=1, n_jobs=-1)

grid_search.fit(X_train, y_train)

best_parameter = grid_search.best_params_
best_parameter

best_model = grid_search.best_estimator_
best_model

#training the RandomForestRegressor using the best parameters from
imporoved_rfregressor = RandomForestRegressor(**best_parameter)

imporoved_rfregressor.fit(X_train, y_train)

scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=scoring)
scores

rmse_score =np.sqrt(-scores)
rmse_score

improved_model_prediction = imporoved_rfregressor.predict(X_test)
pd.DataFrame(improved_model_prediction)

y_test.iloc[100:]

rsme_improved = np.sqrt(mean_squared_error(y_test, improved_model_prediction))
print("Improved RMSE:", rsme_improved)



mae = mean_absolute_error(y_test, improved_model_prediction)
print("mean squared error:", mae)

r2 = r2_score(y_test, improved_model_prediction)
print("r_squared", r2)

imporoved_rfregressor.score(X_test, y_test)



"""## Training - XGBoost Model"""

#using the xgboost regressor
xgboost_initial_model = xgb.XGBRegressor()

xgboost_initial_model.fit(X_train, y_train)

xgboost_initial_prediction = xgboost_initial_model.predict(X_test)
xgboost_initial_prediction

xgboost_initial_rmse = np.sqrt(mean_squared_error(y_test, xgboost_initial_prediction))
print("Initial RMSE:", xgboost_initial_rmse)

#Hyperparameter tuning
xgb_params = {
    'max_depth': [3, 4],
    'learning_rate': [0.1, 0.01],
    'n_estimators': [100, 150],
    'colsample_bytree': [0.8, 0.9],
    'subsample': [0.8, 0.9]
}

xgb_grid_search = GridSearchCV(estimator=xgboost_initial_model, param_grid=xgb_params, scoring=scoring, cv=5, verbose=1, n_jobs=-1)

xgb_grid_search.fit(X_train, y_train)

xgb_best_params = xgb_grid_search.best_params_
xgb_best_params

xgb_best_model = xgb_grid_search.best_estimator_
xgb_best_model

xgb_final_model = xgb.XGBRegressor(**xgb_best_params)

xgb_final_model.fit(X_train, y_train)

xgb_final_prediction = xgb_final_model.predict(X_test)
xgb_final_prediction

xgb_final_rmse = np.sqrt(mean_squared_error(y_test, xgb_final_prediction))
print("Final RMSE:", xgb_final_rmse)

xgb_mae = mean_absolute_error(y_test, xgb_final_prediction)
print("mean squared error:", xgb_mae)

xgb_r2 = r2_score(y_test, xgb_final_prediction)
print("r_squared", xgb_r2)

xgb_final_model.score(X_test, y_test)

"""## Training - Gradient Boost Regression"""

gradient_boost_initial_model = GradientBoostingRegressor()

gradient_boost_initial_model.fit(X_train, y_train)

gradient_boost_predict = gradient_boost_initial_model.predict(X_test)
gradient_boost_predict

gradient_boost_initial_rmse = np.sqrt(mean_squared_error(y_test, gradient_boost_predict))
print("Initial RMSE:", gradient_boost_initial_rmse)

gradient_boost_params = {
    'n_estimators': [100, 150],
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 4],
    'min_samples_split': [2, 3],
    'min_samples_leaf': [1, 2]
}

gradient_boost_grid_search = GridSearchCV(estimator=gradient_boost_initial_model, param_grid=gradient_boost_params, scoring=scoring, cv=5, verbose=1, n_jobs=-1)

gradient_boost_grid_search.fit(X_train, y_train)

gradient_boost_best_params = gradient_boost_grid_search.best_params_
gradient_boost_best_params

gradient_boost_best_model = gradient_boost_grid_search.best_estimator_
gradient_boost_best_model

gradient_boost_final_model = GradientBoostingRegressor(**gradient_boost_best_params)

gradient_boost_final_model.fit(X_train, y_train)

gradient_boost_final_prediction = gradient_boost_final_model.predict(X_test)
gradient_boost_final_prediction

gradient_boost_final_rmse = np.sqrt(mean_squared_error(y_test, gradient_boost_final_prediction))
print("Final RMSE:", gradient_boost_final_rmse)

gradient_boost_mae = mean_absolute_error(y_test, gradient_boost_final_prediction)
print("mean squared error:", gradient_boost_mae)

gradient_boost_r2 = r2_score(y_test, gradient_boost_final_prediction)
print("r_squared", gradient_boost_r2)

gradient_boost_score = gradient_boost_final_model.score(X_test, y_test)
gradient_boost_score



"""# Model Scores

1.   Random Forest Regressor: 0.9595781026460589
2.   Xgboost Regressor:       0.9521185930996207
3.   Gradient Boost Regresor: 0.9517684582390373

# Training model with unseen data:players_22-1 dataset

***Cleaning new data***
"""

players_22 = pd.read_csv('/content/drive/My Drive/FIFA23_PREDICTIONMODEL/players_22-1.csv')

pd.set_option('display.max_columns', None)

players_22.info()

players_22.head()

players_22.columns

players_22.describe()

print('The shape of this dataset is',players_22.shape[1])

threshold_percentage = 0.2  # 20% of the total rows
min_non_null_values = int(threshold_percentage * players_22.shape[0])
players_22.dropna(axis=1, thresh=min_non_null_values, inplace=True)

print('The shape of this dataset is',players_22.shape[1])

unnecessary_columns = [
    "player_url",
    "sofifa_id",
    "dob",
    "club_team_id",
    "long_name",
    "real_face",
    "body_type",
    "club_jersey_number",
    "club_joined",
    "nationality_id",
    "player_face_url",
    "club_logo_url",
    "club_flag_url",
    "nation_flag_url"
]

print(unnecessary_columns)

new_22 = players_22.drop(columns=unnecessary_columns)

print(new_22.columns)

new_22.info()

categorical_data1 = pd.DataFrame()
numerical_data1 = pd.DataFrame()

for columns in new_22.columns:
    if new_22[columns].dtype == 'object':
        categorical_data1[columns] = new_22[columns]
    else:
        numerical_data1[columns] = new_22[columns]

categorical_data1.info()

numerical_data1.info()

print(numerical_data1.isnull().sum())

numerical_data1.ffill(axis=0, inplace=True)

print(numerical_data1.isnull().sum())

categorical_data1.ffill(axis=0, inplace=True)

print(categorical_data1.isnull().sum())

encoded_categroical_data1 = categorical_data1.copy()

for columns in categorical_data1.columns:
    encoded_categroical_data1[columns] = encoder.fit_transform(categorical_data1[columns])

encoded_categroical_data1.info()

encoded_categroical_data1

clean_players_22_data = pd.concat([numerical_data1, encoded_categroical_data1], axis=1)

clean_players_22_data

selected_features

features = clean_players_22_data[selected_features]

features

"""# Testing with new data"""

original_data = clean_players_22_data['overall']

features_scaled = scaler.transform(features)

random_forest_prediction = pd.DataFrame(imporoved_rfregressor.predict(features_scaled))
random_forest_prediction

xgboost_prediction = pd.DataFrame(xgb_final_model.predict(features_scaled))
xgboost_prediction

gradient_boost_prediction = pd.DataFrame(gradient_boost_final_model.predict(features_scaled))
gradient_boost_prediction

rmse_random_forest = np.sqrt(mean_squared_error(original_data, random_forest_prediction))
print("Random Forest RMSE:", rmse_random_forest)

rmse_xgboost = np.sqrt(mean_squared_error(original_data, xgboost_prediction))
print("XGBoost RMSE:", rmse_xgboost)

rmse_gradient_boost = np.sqrt(mean_squared_error(original_data, gradient_boost_prediction))
print("Gradient Boost RMSE:", rmse_gradient_boost)

"""# Saving chosen model:

Chosen Model: ***Random Forest Regressor***
"""

saved_directory = '/content/drive/My Drive/FIFA23_PREDICTIONMODEL/rfregressor.pk1'
with open(saved_directory, 'wb') as file:
    pickle.dump(imporoved_rfregressor, file)

